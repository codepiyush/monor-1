{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "61aec30a-f65c-02dd-77b0-67aec83e8632",
    "_execution_state": "idle",
    "_uuid": "6ce895e9b32b7216d59f64c276b7d69068b287e8"
   },
   "outputs": [],
   "source": [
    "#We import libraries for linear algebra, graphs, and evaluation of results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.ndimage.filters import uniform_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d4f7e51a-b964-8eaf-220b-7a924aa7932e",
    "_execution_state": "idle",
    "_uuid": "3dab37c32ac2c1edef4bae4fc3534b92355911ae"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-503f1c624145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Keras is a high level neural networks library, based on either tensorflow or theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPool1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Keras is a high level neural networks library, based on either tensorflow or theano\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten, \\\n",
    "BatchNormalization, Input, concatenate, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3da338c-5beb-d12c-9cb4-088c1eb86069",
    "_uuid": "68a1f6dba3b023e7eb59c6983fdde57851c030ce"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "68ec0ee6eb9d8256998b7c15d37ec62b3e61eb5b"
   },
   "source": [
    "As the data format is so simple, we do not need pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2763eb6d-1867-7254-7278-099e63ddd562",
    "_execution_state": "idle",
    "_uuid": "1cfdd50387bac260912e23444d1234540d5c1b02"
   },
   "outputs": [],
   "source": [
    "INPUT_LIB = '../input/'\n",
    "raw_data = np.loadtxt(INPUT_LIB + 'exoTrain.csv', skiprows=1, delimiter=',')\n",
    "x_train = raw_data[:, 1:]\n",
    "y_train = raw_data[:, 0, np.newaxis] - 1.\n",
    "raw_data = np.loadtxt(INPUT_LIB + 'exoTest.csv', skiprows=1, delimiter=',')\n",
    "x_test = raw_data[:, 1:]\n",
    "y_test = raw_data[:, 0, np.newaxis] - 1.\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "a0b077fb1c89e5afe853f137ded82ffbfcb0449f"
   },
   "source": [
    "Scale each observation to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "17cbb5bb-29d7-9a0c-193c-9c132cdea389",
    "_execution_state": "idle",
    "_uuid": "76dd566fee56a9feaf06b9cff2fa54f5239e33cb"
   },
   "outputs": [],
   "source": [
    "x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) / \n",
    "           np.std(x_train, axis=1).reshape(-1,1))\n",
    "x_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) / \n",
    "          np.std(x_test, axis=1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "e2aa36f3133a1fad819e48dc60e120aed6283ce2"
   },
   "source": [
    "This is our only preprocessing step: We add an input corresponding to the running average over\n",
    "200 time steps. This helps the net ignore high frequency noise and instead look at non-local\n",
    "information. Look at the graphs below to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c006e96-8688-72e4-6abe-65d5d8f22fc6",
    "_execution_state": "busy",
    "_uuid": "f4d354b6d9d6b2fb5c788355c99e61a62699323c"
   },
   "outputs": [],
   "source": [
    "x_train = np.stack([x_train, uniform_filter1d(x_train, axis=1, size=200)], axis=2)\n",
    "x_test = np.stack([x_test, uniform_filter1d(x_test, axis=1, size=200)], axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c90d56a7-dec3-65a3-cac3-32e36c6bf31c",
    "_uuid": "6651259e2b8bab4c98f4649263e583201e494e45"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "a6eeb9020d7eae0925578490c692217659cd4a6b"
   },
   "source": [
    "With the Sequential API for Keras, we only need to add the layers one at a time. Each 1D convolutional layers corresponds to a local filter, and then a pooling layer reduces the data length by approximately a factor 4. At the end, there are two dense layers, just as we would in a typical image classifier. Batch normalization layers speed up convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3637a7be-c21b-3b78-b6b6-9146332f3972",
    "_execution_state": "busy",
    "_uuid": "bbbee746cbd798b62d349a2b4d828e2fb889e164"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=8, kernel_size=11, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=16, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "5d25c5ad8a6ddd1319af7c56075a05fbfec8749b"
   },
   "source": [
    "The data here is extremely unbalanced, with only a few positive examples. To correct for this, I use the positive examples a lot more often, so that the net sees 50% of each over each bats. Also, I generate new examples by rotation them randomly in time. This is called augmentation and is similar to when we rotate/shift examples in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ba8b520-7302-747c-7f84-8a816508e651",
    "_execution_state": "busy",
    "_uuid": "0604716b9a92c9113dcd16631b6fc48ff006fd53"
   },
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size=32):\n",
    "    \"\"\"\n",
    "    Gives equal number of positive and negative samples, and rotates them randomly in time\n",
    "    \"\"\"\n",
    "    half_batch = batch_size // 2\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
    "    \n",
    "    yes_idx = np.where(y_train[:,0] == 1.)[0]\n",
    "    non_idx = np.where(y_train[:,0] == 0.)[0]\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(yes_idx)\n",
    "        np.random.shuffle(non_idx)\n",
    "    \n",
    "        x_batch[:half_batch] = x_train[yes_idx[:half_batch]]\n",
    "        x_batch[half_batch:] = x_train[non_idx[half_batch:batch_size]]\n",
    "        y_batch[:half_batch] = y_train[yes_idx[:half_batch]]\n",
    "        y_batch[half_batch:] = y_train[non_idx[half_batch:batch_size]]\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "     \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "fd043b67ebf19352b1b8ec2a063ad8a50765e70d"
   },
   "source": [
    "The hyperparameters here are chosen to finish training within the Kernel, rather than to get optimal results. On a GPU, I might have chosen a smaller learning rate, and perhaps SGD instead of Adam. As it turned out, results were brilliant anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe6afcb2-7e0a-501d-bf19-759c0b3fda77",
    "_execution_state": "idle",
    "_uuid": "c8844e546b1d4677b3be321cbc203c8c617ef7c4"
   },
   "outputs": [],
   "source": [
    "#Start with a slightly lower learning rate, to ensure convergence\n",
    "model.compile(optimizer=Adam(1e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit_generator(batch_generator(x_train, y_train, 32), \n",
    "                           validation_data=(x_test, y_test), \n",
    "                           verbose=0, epochs=5,\n",
    "                           steps_per_epoch=x_train.shape[1]//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "964bdbf2-a325-11da-98c8-4dd56b31c9ad",
    "_execution_state": "busy",
    "_uuid": "6030dfe0cb763066fdecf81a27dff897cce69af4"
   },
   "outputs": [],
   "source": [
    "#Then speed things up a little\n",
    "model.compile(optimizer=Adam(4e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit_generator(batch_generator(x_train, y_train, 32), \n",
    "                           validation_data=(x_test, y_test), \n",
    "                           verbose=2, epochs=40,\n",
    "                           steps_per_epoch=x_train.shape[1]//32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "busy",
    "_uuid": "31d3be9df5d7387bf68eb9ca2c1c73c1590c8681"
   },
   "source": [
    "#Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "d8a74931a02d8ceb43c03f9f687f934581e11e9d"
   },
   "source": [
    "First we look at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "020d5851-762a-d02c-77fa-319c3af6a4af",
    "_execution_state": "idle",
    "_uuid": "6c22b94c662b6b25fd33c85fd05892aca54e5d86"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "62f58f451ef2de7c4895f8bdb449dae44a122871"
   },
   "source": [
    "We then use our trained net to classify the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ef32235-eebb-91e2-4571-1b585aa86ad4",
    "_execution_state": "idle",
    "_uuid": "62e80b4160b0bfd27bf1acdb2e5b6baed971fc01"
   },
   "outputs": [],
   "source": [
    "non_idx = np.where(y_test[:,0] == 0.)[0]\n",
    "yes_idx = np.where(y_test[:,0] == 1.)[0]\n",
    "y_hat = model.predict(x_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f37a9af-ceab-8b5a-3f77-fefd6eb38710",
    "_execution_state": "idle",
    "_uuid": "4b002ba89846d5f0c25fd29466c9ceb4801fc7e5"
   },
   "outputs": [],
   "source": [
    "plt.plot([y_hat[i] for i in yes_idx], 'bo')\n",
    "plt.show()\n",
    "plt.plot([y_hat[i] for i in non_idx], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "ffabc4d6783302adc67cf15ac6ca038030ff8842"
   },
   "source": [
    "These graphs show that the five positive examples all get 0.95-1.00 score. Also, almost all negative examples get score close to zero, except a few in the 0.9-1.0 range. This is encouraging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "088fa46309212bf4a02766ea44e918d561643718"
   },
   "source": [
    "We now choose an optimal cutoff score for classification. Sklearn can help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3d10d764-1a5f-7315-ea45-9e9481261ea4",
    "_execution_state": "busy",
    "_uuid": "38e7322f333dc2afa38186f208e5b96f4bd83fb9"
   },
   "outputs": [],
   "source": [
    "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_hat)\n",
    "plt.plot(thresholds, 1.-fpr)\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.show()\n",
    "crossover_index = np.min(np.where(1.-fpr <= tpr))\n",
    "crossover_cutoff = thresholds[crossover_index]\n",
    "crossover_specificity = 1.-fpr[crossover_index]\n",
    "print(\"Crossover at {0:.2f} with specificity {1:.2f}\".format(crossover_cutoff, crossover_specificity))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()\n",
    "print(\"ROC area under curve is {0:.2f}\".format(roc_auc_score(y_true, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67e27578-3029-3e51-448f-bc2b7a8f24ab",
    "_uuid": "d15012aa5570172dee9f99f79dc962a06aedd63c"
   },
   "source": [
    "Let's take a look at the misclassified data (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28f895b0-597b-4f46-1a7f-7377d97ade05",
    "_execution_state": "idle",
    "_uuid": "ce60fdc5bcf2b2d08f68c22d581116383d6b6c45"
   },
   "outputs": [],
   "source": [
    "false_positives = np.where(y_hat * (1. - y_test) > 0.5)[0]\n",
    "for i in non_idx:\n",
    "    if y_hat[i] > crossover_cutoff:\n",
    "        print(i)\n",
    "        plt.plot(x_test[i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c3c8bbf-9638-8f4f-050e-695a66a8b84a",
    "_uuid": "84ee7f7a1f17f0c2d5f309fa957a0cb8bbcd38ba"
   },
   "source": [
    "It seems NASA missed one planet. I take this opportunity to claim it, and hereby name it Kaggle alpha :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0bebfd57-3ff5-90d1-79a2-4fb500116cc6",
    "_execution_state": "idle",
    "_uuid": "0e60c2111bc64417f9320ecb5f6adab298423606"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
